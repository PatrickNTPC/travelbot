# streamlit run streamlit_travelbot.py
# 3å¤©2å¤œå˜‰ç¾©éŠ

import os
import streamlit as st
from google import genai
from google.genai import types

import logging
import sys

# ----------------------------------------------------
# 1. é…ç½®é é¢å’Œæ¨™é¡Œ
# ----------------------------------------------------
st.set_page_config(page_title="ğŸ¤– æ™ºèƒ½æ—…éŠè¦åŠƒå¸« (Gemini API)", layout="wide")
st.title("ğŸ—ºï¸ æ‚¨çš„å°ˆå±¬æ—…éŠè¦åŠƒæ©Ÿå™¨äºº")

# ç”±æ–¼åº•å±¤ SDK å¯èƒ½ç¹é Python Loggersï¼Œæˆ‘å€‘å°‡ä¸å†ä¾è³´ logging æ¨¡çµ„
# åƒ…ç¢ºä¿æ ¸å¿ƒé…ç½®åœ¨æœ€ä¸Šæ–¹

# ----------------------------------------------------
# 2. æ ¸å¿ƒé…ç½®
# ----------------------------------------------------

# ç³»çµ±æŒ‡ä»¤ (System Instruction)
SYSTEM_INSTRUCTION_TEXT = (
    "ä½ æ˜¯ä¸€ä½é ‚ç´šçš„æ—…éŠè¦åŠƒå¸«ï¼Œå°ˆç²¾æ–¼äºæ´²æ–‡åŒ–æ·±åº¦æ—…è¡Œã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šç”¨æˆ¶çš„éœ€æ±‚ï¼Œæä¾›åŒ…å«ã€Œæ™¯é»ã€ã€ã€Œç¾é£Ÿã€å’Œã€Œäº¤é€šã€çš„è©³ç´°å»ºè­°ã€‚"
    "å›ç­”é¢¨æ ¼å¿…é ˆæ˜¯ç†±æƒ…ã€å°ˆæ¥­ä¸”å¯Œæœ‰å€‹äººè¦‹è§£çš„ã€‚è«‹ç¢ºä¿æ¯ä¸€æ¬¡çš„å›è¦†éƒ½åƒä¸€ç¯‡ç²¾ç¾çš„å°æ–‡ç« ï¼Œä¸¦ä»¥æ¢åˆ—å¼é‡é»çµå°¾ã€‚"
)

MODEL_NAME = "gemini-2.5-flash"  # ä½¿ç”¨ç©©å®šä¸”é«˜æ•ˆçš„ flash æ¨¡å‹
TEMPERATURE_VALUE = 0.6
MAX_TOKENS = 65535

# ----------------------------------------------------
# 3. ç²å– API é‡‘é‘° (å®‰å…¨ä¸”äº’å‹•å¼)
# ----------------------------------------------------
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    GEMINI_API_KEY = st.sidebar.text_input(
        "è«‹è¼¸å…¥æ‚¨çš„ GEMINI_API_KEY", 
        type="password", 
        help="é‡‘é‘°ä¸æœƒè¢«å„²å­˜ã€‚æ‚¨å¯ä»¥åœ¨ Google AI Studio ç²å–ã€‚"
    )

if not GEMINI_API_KEY:
    st.info("è«‹å…ˆåœ¨å·¦å´é‚Šæ¬„è¼¸å…¥æ‚¨çš„ API é‡‘é‘°ä»¥ç¹¼çºŒã€‚")
    st.stop()


# ----------------------------------------------------
# 4. åˆå§‹åŒ– Gemini å®¢æˆ¶ç«¯å’Œé…ç½® (é‡è¦ä¿®æ­£)
# ----------------------------------------------------

# å®šç¾©å·¥å…· (Google Search)
tools = [
    types.Tool(googleSearch=types.GoogleSearch()),
]

# å®šç¾©ç”Ÿæˆå…§å®¹çš„é…ç½®
config = types.GenerateContentConfig(
    temperature=TEMPERATURE_VALUE,
    max_output_tokens=MAX_TOKENS,
    tools=tools,
    system_instruction=SYSTEM_INSTRUCTION_TEXT,
)

# åƒ…åˆå§‹åŒ– clientï¼Œä¸¦ç§»é™¤ Chat ç‰©ä»¶ï¼Œé¿å…èª¿è©¦è¼¸å‡º
if "client" not in st.session_state:
    st.session_state.messages = []
    
    try:
        # åƒ…åˆå§‹åŒ– client
        st.session_state.client = genai.Client(api_key=GEMINI_API_KEY)
        
    except Exception as e:
        st.error(f"åˆå§‹åŒ– Gemini å¤±æ•—ï¼š{e}")
        st.info("è«‹æª¢æŸ¥æ‚¨çš„ API é‡‘é‘°å’Œç¶²è·¯é€£ç·šã€‚")
        st.stop()


# ----------------------------------------------------
# 5. é¡¯ç¤ºèŠå¤©è¨˜éŒ„
# ----------------------------------------------------
# é¡¯ç¤ºæ­·å²è¨Šæ¯
for message in st.session_state.messages:
    # 'role' æ˜¯ 'user' æˆ– 'model'
    role = "user" if message["role"] == "user" else "assistant"
    with st.chat_message(role):
        st.markdown(message["content"])

# ----------------------------------------------------
# 6. è™•ç†ç”¨æˆ¶è¼¸å…¥å’Œ API å‘¼å« (æ‰‹å‹•è¿­ä»£ä¸¦éæ¿¾)
# ----------------------------------------------------
if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„æ—…éŠè¦åŠƒéœ€æ±‚..."):
    
    # å„²å­˜ä¸¦é¡¯ç¤ºç”¨æˆ¶è¼¸å…¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # å‘¼å« Gemini API
    with st.chat_message("assistant"):
        with st.spinner("æ—…éŠè¦åŠƒå¸«æ­£åœ¨åŠªåŠ›æ€è€ƒä¸­..."):
            
            # ä¿®æ­£ï¼šå°‡ Streamlit è¨Šæ¯æ­·å²è½‰æ›ç‚º API å…§å®¹åˆ—è¡¨
            api_contents = []
            for msg in st.session_state.messages:
                # API å…§å®¹éœ€è¦ 'role' ('user' æˆ– 'model') å’Œ 'parts'
                # æ³¨æ„ï¼šStreamlit çš„ role 'assistant' å¿…é ˆè½‰æ›ç‚º API çš„ 'model'
                role = "user" if msg["role"] == "user" else "model"
                api_contents.append(
                    types.Content(
                        role=role,
                        # ä¿®æ­£: ä½¿ç”¨ types.Part(text=...) ç¢ºä¿ç‰ˆæœ¬å…¼å®¹æ€§
                        parts=[types.Part(text=msg["content"])]
                    )
                )

            # ä¿®æ­£ï¼šä½¿ç”¨ generate_content_stream é€²è¡Œä¸²æµå‘¼å«
            response_generator = st.session_state.client.models.generate_content_stream(
                model=MODEL_NAME,
                contents=api_contents, # å‚³éå®Œæ•´çš„èŠå¤©æ­·å²
                config=config          # å‚³éé…ç½® (åŒ…å«ç³»çµ±æŒ‡ä»¤å’Œå·¥å…·)
            )

            # ğŸ’¡ æœ€çµ‚ä¿®æ­£ï¼šæ‰‹å‹•è¿­ä»£ç”Ÿæˆå™¨ï¼Œåªæ“·å–ä¸¦è¼¸å‡º .text å…§å®¹
            full_response = ""
            message_placeholder = st.empty() 

            # è¿­ä»£ç”Ÿæˆå™¨ï¼Œåªè™•ç†å¸¶æœ‰æ–‡æœ¬å…§å®¹çš„å¡Š
            for chunk in response_generator:
                # ä½¿ç”¨ try-except ç¢ºä¿é‡åˆ°éæ¨™æº–çš„èª¿è©¦ç‰©ä»¶æ™‚ä¸æœƒå´©æ½°
                try:
                    # åªæœ‰ Content å¡Šæ‰æœƒæœ‰ .text å±¬æ€§
                    if hasattr(chunk, 'text'):
                        text_to_print = chunk.text
                        
                        if text_to_print:
                            full_response += text_to_print
                            # å¯¦æ™‚æ›´æ–°ä½”ä½ç¬¦ä¸¦æ·»åŠ æ¸¸æ¨™
                            message_placeholder.markdown(full_response + "â–Œ")
                except Exception:
                    # å¿½ç•¥æ‰€æœ‰ç„¡æ³•æ­£ç¢ºè§£æç‚ºæ–‡æœ¬çš„ç‰©ä»¶ï¼ˆä¾‹å¦‚ sdk_http_response ç›¸é—œçš„èª¿è©¦ç‰©ä»¶ï¼‰
                    continue 
            
            # ç§»é™¤é–ƒçˆæ¸¸æ¨™ï¼Œé¡¯ç¤ºæœ€çµ‚å®Œæ•´å›è¦†
            message_placeholder.markdown(full_response)
            
    # å°‡åŠ©ç†çš„å›è¦†æ·»åŠ åˆ°èŠå¤©è¨˜éŒ„ä¸­
    st.session_state.messages.append({"role": "model", "content": full_response})